{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resuming Training\n",
    "\n",
    "When using Trainer locally all the model checkpoints are saved locally, however we can design a solution to sync checkpoints to external storages.\n",
    "This notebook shows a way to upload checkpoints to either an s3 bucket or a GCS bucket, but can be extended for more storage options.\n",
    "It also include an example code to retrieve checkpoints from those storage in order to resume training.\n",
    "\n",
    "\n",
    "1) [Preparation](#preparation)\n",
    "\n",
    "2) [Checkpoint Callbacks](#checkpoint-callbacks)\n",
    "\n",
    "3) [Util checkpoint downloader from storages](#checkpoint-downloader)\n",
    "\n",
    "4) [Model training example](#model-training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Let's first install the required packages and import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import (AutoTokenizer, \n",
    "                          DataCollatorWithPadding,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          TrainerCallback,\n",
    "                          AutoModelForSequenceClassification)\n",
    "\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import AWS and GCP libraries\n",
    "import boto3\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define some variables used throughout the notebook. You can change those to suit your needs. \n",
    "\n",
    "Note that you need to create buckets beforehand (check the links in the README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 and GCS clients\n",
    "s3_client = boto3.client('s3')\n",
    "gcs_client = storage.Client(project=\"hf-notebooks\")\n",
    "\n",
    "# Set the training name\n",
    "training_name = \"trainer-demo-checkpointing\"\n",
    "\n",
    "# S3 bucket name (To update)\n",
    "s3_bucket_name = \"hf-demo-s3-checkpointing-sagemaker\"\n",
    "\n",
    "# GCS bucket name (To update)\n",
    "gcs_bucket_name = \"hf-demo-gcs-checkpointing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our first callback. \n",
    "\n",
    "It will save the model checkpoints to an S3 bucket everytime we save those locally (more info on the [trainer callback and the on save method](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/callback#transformers.TrainerCallback.on_save)).\n",
    "We can define how frequent we want those to be uploaded in the training arguments as we will see next. Typically, we can save those every 100 steps like this : \n",
    "\n",
    "```python\n",
    "TrainingArguments(...,\n",
    "                  save_strategy = \"steps\",\n",
    "                  save_steps = 100,\n",
    "                  ...)\n",
    "```\n",
    "\n",
    "We can retrieve the step from the [TrainerState](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/callback#transformers.TrainerState) state argument.\n",
    "Model checkpoints are first saved locally with the name {training_name}/checkpoint-{step}, and we can simply upload them to S3. Or we could save those every 5 times for example to reduce costs : \n",
    "\n",
    "```python\n",
    "if (state.global_step%state.save_step % 5 == 0) : \n",
    "    # Code to save\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpointsToS3Callback(TrainerCallback):\n",
    "    '''\n",
    "    This class is a callback that saves the model checkpoints to S3\n",
    "    '''\n",
    "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        '''\n",
    "        Function called when the model is saved. It uploads the model checkpoint to S3\n",
    "        '''\n",
    "\n",
    "        # Get checkpoint folder\n",
    "        model_checkpoint = \"{}/checkpoint-{}\".format(training_name, state.global_step)\n",
    "        \n",
    "        # Upload all the checkpoint files to the S3\n",
    "        for filename in os.listdir(model_checkpoint):\n",
    "            filename_path = \"/\".join([model_checkpoint,filename])\n",
    "            s3_client.upload_file(filename_path, s3_bucket_name, filename_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second callback is quite similar, except that we now upload to a GCP bucket.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveCheckpointsToGCSCallback(TrainerCallback):\n",
    "    '''\n",
    "    This class is a callback that saves the model checkpoints to GCS\n",
    "    '''\n",
    "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        '''\n",
    "        Function called when the model is saved. It uploads the model checkpoint to S3\n",
    "        '''\n",
    "\n",
    "        # Need to install gcloud if not already installed (https://cloud.google.com/sdk/docs/install)\n",
    "        # Need to \"gcloud auth application-default login\" before running this\n",
    "        # Get checkpoint folder\n",
    "        model_checkpoint = \"{}/checkpoint-{}\".format(training_name, state.global_step)\n",
    "        \n",
    "        bucket = gcs_client.get_bucket(gcs_bucket_name)\n",
    "        \n",
    "        \n",
    "        for filename in os.listdir(model_checkpoint):\n",
    "            filename_path = \"/\".join([model_checkpoint,filename])\n",
    "            \n",
    "            blob = bucket.blob(filename_path)\n",
    "            blob.upload_from_filename(filename_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint downloader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python class, CloudCheckpointLoader, is designed to download model checkpoints from either Amazon S3 or Google Cloud Storage (GCS).\n",
    "\n",
    "The ```download_checkpoints``` method is responsible for downloading the checkpoints from the specified bucket. It first checks the bucket_type and then calls the appropriate method to download the checkpoints. \n",
    "\n",
    "The ```__get_last_checkpoint_from_s3``` and ```__get_last_checkpoint_from_gcs``` methods are helper methods used to retrieve the last checkpoint from S3 and GCS respectively. You can change those to retrieve the desired checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudCheckpointLoader():\n",
    "    '''\n",
    "    This class is used to download the model checkpoints from S3 or GCS.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, bucket_type=\"s3\"):\n",
    "        '''\n",
    "        Initializes a new instance of the CloudCheckpointLoader class.\n",
    "\n",
    "        Parameters:\n",
    "            bucket_type (str): The type of bucket to download checkpoints from. Valid values are \"s3\" or \"gcs\".\n",
    "        '''\n",
    "        # Set the bucket type (s3 or gcs)\n",
    "        self.bucket_type = bucket_type\n",
    "\n",
    "        # Check if the bucket type is valid\n",
    "        assert self.bucket_type in [\"s3\", \"gcs\"], \"Invalid bucket type. Please choose either s3 or gcs\"\n",
    "    \n",
    "\n",
    "    def download_checkpoints(self):\n",
    "        '''\n",
    "        Downloads the checkpoints from the specified bucket.\n",
    "\n",
    "        Returns:\n",
    "            str: The path to the downloaded checkpoints.\n",
    "        '''\n",
    "        \n",
    "        if self.bucket_type == \"s3\":\n",
    "\n",
    "            print(\"Downloading checkpoints from S3...\")\n",
    "            checkpoint_dir, checkpoint_files = self.__get_last_checkpoint_from_s3()\n",
    "            path_to_checkpoint = \"/\".join([training_name,checkpoint_dir])\n",
    "            os.makedirs(path_to_checkpoint, exist_ok=True)\n",
    "\n",
    "            for file in tqdm(checkpoint_files):\n",
    "                s3_client.download_file(s3_bucket_name, file, file)\n",
    "\n",
    "\n",
    "        elif self.bucket_type == \"gcs\":\n",
    "\n",
    "            bucket = gcs_client.get_bucket(gcs_bucket_name)\n",
    "\n",
    "            print(\"Downloading checkpoints from GCS...\")\n",
    "            checkpoint_dir, checkpoint_files = self.__get_last_checkpoint_from_gcs(bucket)\n",
    "            path_to_checkpoint = \"/\".join([training_name,checkpoint_dir])\n",
    "            os.makedirs(path_to_checkpoint, exist_ok=True)\n",
    "\n",
    "            for file in tqdm(checkpoint_files):\n",
    "                blob = bucket.blob(file)\n",
    "                blob.download_to_filename(file)\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError(\"Invalid bucket type. Please choose either s3 or gcs\")\n",
    "\n",
    "\n",
    "    def __get_last_checkpoint_from_s3(self):\n",
    "        '''\n",
    "        Retrieves the last checkpoint from S3.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the checkpoint directory and a list of checkpoint files.\n",
    "        '''\n",
    "        # List all the objects in the bucket\n",
    "        response = s3_client.list_objects_v2(Bucket=\"hf-demo-s3-checkpointing-sagemaker\")\n",
    "        \n",
    "        # Sort the objects by the last modified date\n",
    "        sorted_content = sorted(response[\"Contents\"], \n",
    "                                key=lambda obj: int(obj['LastModified'].strftime('%s')))\n",
    "        \n",
    "        # Get the keys of the sorted objects\n",
    "        sorted_keys =  [obj['Key'] for obj in sorted_content]\n",
    "\n",
    "        # Return all files from the last checkpoint\n",
    "        checkpoint_dir = sorted_keys[-1].split(\"/\")[1]\n",
    "        checkpoint_files = [key for key in sorted_keys if checkpoint_dir in key]\n",
    "\n",
    "        # Return the last checkpoint files\n",
    "        return checkpoint_dir, checkpoint_files\n",
    "\n",
    "\n",
    "    def __get_last_checkpoint_from_gcs(self, bucket):\n",
    "        '''\n",
    "        Retrieves the last checkpoint from GCS.\n",
    "\n",
    "        Parameters:\n",
    "            bucket (object): The GCS bucket object.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the checkpoint directory and a list of checkpoint files.\n",
    "        '''\n",
    "        # List all the objects in the bucket\n",
    "        blobs = bucket.list_blobs()\n",
    "        \n",
    "        # Sort the objects by the last modified date\n",
    "        sorted_content = sorted(list(blobs), \n",
    "                                key=lambda obj: int(obj.time_created.strftime('%s')))\n",
    "        \n",
    "        # Get the keys of the sorted objects\n",
    "        sorted_keys =  [obj.name for obj in sorted_content]\n",
    "\n",
    "        # Return all files from the last checkpoint\n",
    "        checkpoint_dir = sorted_keys[-1].split(\"/\")[1]\n",
    "        checkpoint_files = [key for key in sorted_keys if checkpoint_dir in key]\n",
    "\n",
    "        # Return the last checkpoint files\n",
    "        return checkpoint_dir, checkpoint_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example training code below is from the excellent Hugging Face [tutorial](https://huggingface.co/learn/nlp-course/chapter3/3). It serves as an easy guide to understand the code to add our callbacks.\n",
    "\n",
    "We start by preparing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset downloading from the Hub, tokenizing and preparing the data collator\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "                \n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then include our callbacks in the Trainer class, and we can as well start a training by specifying a model checkpoint from the S3 or the GCP bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "# In particular, the save strategy is set to steps and the save steps is set to 100\n",
    "training_args = TrainingArguments(training_name, \n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  save_strategy = \"steps\", \n",
    "                                  save_steps = 100)\n",
    "\n",
    "\n",
    "# Load the model from the hub or a custom checkpoint\n",
    "load_from_checkpoint = False\n",
    "if load_from_checkpoint:\n",
    "\n",
    "    # Download the model from S3\n",
    "    cloud_checkpoint_loader = CloudCheckpointLoader(bucket_type=\"s3\")\n",
    "    custom_checkpoint = cloud_checkpoint_loader.download_checkpoints()\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(custom_checkpoint, \n",
    "                                                               num_labels=2, \n",
    "                                                               local_files_only=True)\n",
    "else:\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, \n",
    "                                                               num_labels=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the checkpoint callbacks\n",
    "save_checkpoints_to_s3 = SaveCheckpointsToS3Callback()\n",
    "save_checkpoints_to_gcs = SaveCheckpointsToGCSCallback()\n",
    "\n",
    "# Add the checkpoint callbacks to the trainer\n",
    "callbacks = [save_checkpoints_to_s3, save_checkpoints_to_gcs]\n",
    "\n",
    "\n",
    "# Define the compute_metrics function for the evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = callbacks\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
